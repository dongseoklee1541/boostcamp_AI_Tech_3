{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c92a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6272fb",
   "metadata": {},
   "source": [
    "많은 용어들을 정확히 정의하고 이해해야 타 연구자들과 커뮤니케이션에서 문제가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52397538",
   "metadata": {},
   "source": [
    "## 5. Modern CNN - 1x1 convolution의 중요성\n",
    "\n",
    "* 파라미터의 숫자, 네트워크의 Depth를 중점으로 보자\n",
    "\n",
    "**네트워크의 Depth는 점점 늘어나고 숫자는 점점 줄어들며 성능은 점점 성장한다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa881fa2",
   "metadata": {},
   "source": [
    "### ILSVRC(ImageNet Large-Scale Visual Recognition Challenge)\n",
    "* Classification / Detection / Localization / Segmentation\n",
    "* 1,000 category"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cded570",
   "metadata": {},
   "source": [
    "### 5-1. AlexNet\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/114450.png?raw=true\" width=\"500\">\n",
    "\n",
    "* 핵심 아이디어\n",
    "    * ReLU(Rectified Linear Unit) activation\n",
    "    * GPU implementation (2 GPUs)\n",
    "    * Local response normalization(지금은 안중요), Overlapping pooling\n",
    "    * Data augmentation\n",
    "    * Dropout\n",
    "\n",
    "\n",
    "### ReLU Activation\n",
    "\n",
    "<img src=\"https://pytorch.org/docs/stable/_images/ReLU.png\" width=\"500\">\n",
    "\n",
    "* Preserves properties of linear models : 0보다 작으면 0 아니면 그대로\n",
    "* Easy to optimize with gradient descent\n",
    "* Good generalization\n",
    "* Overcome the vanishing gradient problem : sigmoid, tanh는 특정 구간에서 gradient가 0에 가까워지는 현상(vanishing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb89258",
   "metadata": {},
   "source": [
    "### 5-2. VGGNet\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/1_AqqArOvacibWqeulyP_-8Q.png?raw=true\" width=\"500\">\n",
    "\n",
    "* **3x3 conv filter만 사용(stride=1)**, AlexNet은 11x11, 3x3을 사용했었다\n",
    "* 1x1 conv\n",
    "* Dropout\n",
    "* VGG16 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab696c3",
   "metadata": {},
   "source": [
    "#### 5-2-1. why 3x3 convolution?\n",
    "conv filter의 크기가 커질수록 이점은, 한번 연산될때 고려할 수 있는 input의 크기가 커진다는 이점이 있다.\n",
    "\n",
    "* Receptive field : 하나의 convolution feature map을 얻기 위해 고려할 수 있는 input의 크기\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/151416.png?raw=true\" width=\"500\">\n",
    "\n",
    "3x3 두번과 5x5 한번은 같은 Receptive field이지만 전자가 파라미터의 갯수가 적다.\n",
    "\n",
    "* 즉 depth를 키우고 파라미터의 숫자를 줄일 수 있다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db78679c",
   "metadata": {},
   "source": [
    "### 5-3.  GoogLeNet\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/googlenet2.png\" width=\"200\">\n",
    "\n",
    "* NIN(network-in-network) : 비슷하게 생긴 네트워크가 계속 반복됨\n",
    "\n",
    "#### inception blocks\n",
    "\n",
    "<img src=\"https://d2l.ai/_images/inception.svg\" width=\"500\">\n",
    "\n",
    "inception은 하나의 Receptive field를 위해 여러개의 필터(1x1 Conv, 3x3 Conv)를 거치고 여러개의 response을 Concatenation 하는 효과가 있다. \n",
    "\n",
    "* 1x1 Conv를 통해서 전체적인 네트워크의 파라미터 수를 줄인다.\n",
    "    * 1x1 Conv는 채널 방향으로 차원을 줄일 수 있다.\n",
    "    \n",
    "#### Benefit of 1x1 Convolution\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/153212.png?raw=True\" width=\"500\">\n",
    "\n",
    "1x1 Conv는 위의 이미지를 기준으로 약 30%의 파라미터 숫자를 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebfe2b2",
   "metadata": {},
   "source": [
    "### 5-4.  ResNet\n",
    "training과 test에서 문제가 되는 generalization gap을 Residual을 통해 해결한 Net\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/154806resnet.png?raw=true\" width=\"500\">\n",
    "\n",
    "* identity map(skip connection) : 출력값 또는 Conv layer에 입력값을 다시 더한다. 그러므로 conv에서 학습하고자 하는 것은 f(x) + x를 하여 차이(residual)만 학습하고자 하는 것이 목적이다.\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/154806resnet2.png?raw=true\" width=\"500\">\n",
    "\n",
    "Residual을 사용하면 더 깊은 layers에서도 학습이 잘되게 해준다.\n",
    "\n",
    "\n",
    "#### Simple/Projected Shortcut\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/simleprojected.png?raw=true\" width=\"500\">\n",
    "\n",
    "* 특이한 점은 Batch Norm이 Conv 다음에 나오게 된다.\n",
    "일반적으로 Simple Shortcut을 많이 쓴다.\n",
    "\n",
    "#### Bottleneck architeture\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/bottleneck.png?raw=true\" width=\"500\">\n",
    "\n",
    "전체적인 파라미터 숫자를 줄이기 위해 input #(Channel)을 줄이고, 3x3 Conv 다음에 #를 늘려 output #의 크기를 맞춰준다.\n",
    "\n",
    "* input과 ouput의 #의 크기를 같게 만들어줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67228a",
   "metadata": {},
   "source": [
    "### 5-5. DenseNet\n",
    "<img src=\"https://raw.githubusercontent.com/dongseoklee1541/boostcamp_AI_Tech_3/9891cd4eeca5ac3e710550cfb1c06a7972f3087f/images/densenet-block.svg\" width=\"500\">\n",
    "\n",
    "두개의 값을 더하는 하는 ResNet과 달리 DenseNet은 Concatenation을 한다.\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/180911.png?raw=true\" width=\"500\">\n",
    "\n",
    "다만 이 경우 채널의 갯수가 점점 커지게 되어 파라미터 숫자가 증가하게 된다.\n",
    "\n",
    "\n",
    "#### 해결방법\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/181157.png?raw=true\" width=\"500\">\n",
    "\n",
    "* BatchNorm - 1x1 Conv - 2x2 AvgPooling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db1f30",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* VGG : 3x3 blocks 반복 , 파라미터 감소\n",
    "* GoogLeNet : 1x1 convolution , 파라미터 감소\n",
    "* ResNet : skip-connection . 네트워크를 깊게 쌓을 수 있게\n",
    "* DenseNet : concatenation , 더하는것 대신 쌓아서 성능을 좋게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a73baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f2aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
