{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c92a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6272fb",
   "metadata": {},
   "source": [
    "많은 용어들을 정확히 정의하고 이해해야 타 연구자들과 커뮤니케이션에서 문제가 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52397538",
   "metadata": {},
   "source": [
    "## 7. Sequential Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa881fa2",
   "metadata": {},
   "source": [
    "### 7-1. Navie sequence model\n",
    "\n",
    "sequence model의 어려움은 데이터의 끝이 정해져 있지 않다는 점이다.\n",
    "\n",
    "#### 7-1-1. Autoregressive model\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/145717.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "그래서 데이터의 끝을 정해서 과거의 일정한 시점까지만 fix하면 더 나아진다.\n",
    "\n",
    "#### 7-1-2. Markov model (first-order autoregressive model)\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/145942.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "Markov model은 바로 직전에 있던 시점에 있는 정보만 현재에 영향을 끼친다는 model\n",
    "\n",
    "* 하지만 말이 안됨, 수능전날 공부한 것 만이 수능에 영향을 미치는가?\n",
    "\n",
    "#### 7-1-3. Latent autoregressive model\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/150037.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "* Hidden/latent state : 과거의 정보를 요약한 것들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf55451",
   "metadata": {},
   "source": [
    "### 7-2. Recurrent Neural Network\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/150625.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "RNN은 현재시간 t의 정보는 이전시간(t-1, t-2 ...)의 정보들이 모두 합쳐져 쭉 이어진다.\n",
    "\n",
    "* RNN을 시간순으로 쭉 풀게되면 입력이 굉장히 많은 Fully Connected Layer로 만들 수 있다.\n",
    "\n",
    "### RNN의 단점들\n",
    "\n",
    "#### 7-2-1. Short, Long-term dependencies\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/151133.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/151818.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "과거의 정보들이 모두 더해져 진행되는데 RNN은 아주 먼 과거의 정보들은 중첩되는 구조로 미래까지 살아남기 힘들어진다(Vanishing / exploding gradient)는 단점이 있다.\n",
    "\n",
    "* 음성인식에서 문장이 길어지면 길어진 문장은 모두 고려하기 어렵다.\n",
    "* 활성함수가 sigmoid의 경우 0과 1사이로 계속 압축시키기에 Vanising, LeRU인 경우는 Exploding하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb98b47",
   "metadata": {},
   "source": [
    "### 7-3. LSTM(Long Short Term Memory)\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/152355.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/152505.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "* LSTM의 입력 : Input , Previous cell state , Previous hidden state\n",
    "\n",
    "* Previous cell state : 0부터 t-1개의 데이터를 취합한 정보, 나가지 않는다. \n",
    "\n",
    "* Previous hidden state : 이전의 데이터를 취합한 정보, output으로 나가면서 다음 네트워크로 흘러간다.\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/153150.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "\n",
    "* Forget gate : 이전 cell state 정보를 **Input , Previous cell state을 통해서 어떤걸 버릴지를 결정하는 역할**\n",
    "\n",
    "* Input gate\n",
    "    * ${i_t}$ : 정보를 추가할지 선택\n",
    "    * ${\\tilde{C}_t}$ : 추가할 정보가 무엇인지 \n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/154113.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "* Output gate : 어떻게 output을 내보낼지 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0230206",
   "metadata": {},
   "source": [
    "### 7-4. GRU(Gated Recurrent Unit)\n",
    "\n",
    "<img src=\"https://github.com/dongseoklee1541/boostcamp_AI_Tech_3/blob/main/images/154605.png?raw=ture\" width=\"500\"> \n",
    "\n",
    "cell state가 없고 hidden state만 가지고 활용, 일반적으로 GRU가 LSTM보다 성능이 좋더라..\n",
    "\n",
    "* 파라미터 숫자가 더 적어서 그러지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d914a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
